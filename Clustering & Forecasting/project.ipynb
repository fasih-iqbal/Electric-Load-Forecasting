{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c27c40a7",
   "metadata": {},
   "source": [
    "Clustering Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90d56356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN: 5859 clusters, 79528 noise points\n",
      "Silhouette Scores:\n",
      "K-Means: 0.138\n",
      "Hierarchical (subsample): 0.111\n",
      "DBSCAN: -0.2994783818721771\n",
      "Cluster Characteristics:\n",
      "               demand  temperature  humidity       hour     month\n",
      "cluster                                                          \n",
      "0         7309.269531     0.470318  0.909456  12.467238  6.960685\n",
      "1        10837.691406     0.701908  0.516355  14.255876  7.481997\n",
      "2         6473.314453     0.509914  0.741092   9.600948  9.791533\n",
      "3         6494.676270     0.458724  0.685714  10.840166  2.657360\n",
      "Cluster 0:\n",
      "- Average Demand: 7309\n",
      "- Average Temperature: 0.47\n",
      "- Average Hour: 12.5\n",
      "- Average Month: 7.0\n",
      "- Mixed patterns\n",
      "\n",
      "Cluster 1:\n",
      "- Average Demand: 10838\n",
      "- Average Temperature: 0.70\n",
      "- Average Hour: 14.3\n",
      "- Average Month: 7.5\n",
      "- Likely: High-demand hot afternoons\n",
      "\n",
      "Cluster 2:\n",
      "- Average Demand: 6473\n",
      "- Average Temperature: 0.51\n",
      "- Average Hour: 9.6\n",
      "- Average Month: 9.8\n",
      "- Mixed patterns\n",
      "\n",
      "Cluster 3:\n",
      "- Average Demand: 6495\n",
      "- Average Temperature: 0.46\n",
      "- Average Hour: 10.8\n",
      "- Average Month: 2.7\n",
      "- Mixed patterns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('processed_data.csv')\n",
    "\n",
    "# Optimize data types to reduce memory\n",
    "df = df.astype(\n",
    "    {col: 'float32' for col in df.select_dtypes(include='float64').columns})\n",
    "\n",
    "# Select features for clustering\n",
    "features = ['demand', 'precipIntensity', 'precipProbability', 'temperature', 'humidity',\n",
    "            'pressure', 'windSpeed', 'hour', 'day_of_week', 'month', 'year', 'anomaly']\n",
    "X = df[features]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X).astype(\n",
    "    np.float32)  # Use float32 to save memory\n",
    "\n",
    "# 1. Dimensionality Reduction\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# t-SNE (subsample for speed)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=250)\n",
    "# Reduced subsample for faster computation\n",
    "X_tsne = tsne.fit_transform(X_scaled[:5000])\n",
    "\n",
    "# Visualize PCA and t-SNE\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.3, s=10)\n",
    "plt.title(f'PCA (Explained Variance: {explained_variance.sum():.2%})')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.3, s=10)\n",
    "plt.title('t-SNE')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.tight_layout()\n",
    "plt.savefig('dimensionality_reduction.png')\n",
    "plt.close()\n",
    "\n",
    "# 2. Clustering Algorithms\n",
    "# K-Means: Elbow Method\n",
    "inertia = []\n",
    "K = range(1, 10)  # Reduced range for speed\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(K, inertia, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.savefig('elbow_curve.png')\n",
    "plt.close()\n",
    "\n",
    "optimal_k = 4  # Adjust based on elbow curve\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "print(f'DBSCAN: {n_clusters_dbscan} clusters, {n_noise} noise points')\n",
    "\n",
    "# Hierarchical Clustering (subsample to avoid memory error)\n",
    "sample_size = 1000  # Optimized for memory\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(\n",
    "    X_scaled.shape[0], sample_size, replace=False)\n",
    "X_sample = X_scaled[sample_indices]\n",
    "\n",
    "# Dendrogram\n",
    "Z = linkage(X_sample, method='ward', metric='euclidean')\n",
    "plt.figure(figsize=(8, 4))\n",
    "dendrogram(Z, truncate_mode='level', p=5)\n",
    "plt.title('Dendrogram for Hierarchical Clustering')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.savefig('dendrogram.png')\n",
    "plt.close()\n",
    "\n",
    "# Apply hierarchical clustering to subsample\n",
    "hierarchical = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
    "hierarchical_labels_sample = hierarchical.fit_predict(X_sample)\n",
    "\n",
    "# 3. Evaluation\n",
    "kmeans_silhouette = silhouette_score(\n",
    "    X_scaled, kmeans_labels, sample_size=1000)  # Subsample for speed\n",
    "hierarchical_silhouette = silhouette_score(\n",
    "    X_sample, hierarchical_labels_sample)\n",
    "dbscan_silhouette = None\n",
    "if n_clusters_dbscan > 1:\n",
    "    dbscan_silhouette = silhouette_score(\n",
    "        X_scaled, dbscan_labels, sample_size=1000)\n",
    "\n",
    "print(f'Silhouette Scores:')\n",
    "print(f'K-Means: {kmeans_silhouette:.3f}')\n",
    "print(f'Hierarchical (subsample): {hierarchical_silhouette:.3f}')\n",
    "print(f'DBSCAN: {dbscan_silhouette if dbscan_silhouette else \"N/A\"}')\n",
    "\n",
    "# Visualize K-Means clusters in PCA space\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels,\n",
    "            cmap='viridis', alpha=0.3, s=10)\n",
    "plt.title('K-Means Clusters in PCA Space')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.savefig('kmeans_clusters.png')\n",
    "plt.close()\n",
    "\n",
    "# 4. Cluster Interpretation\n",
    "df['cluster'] = kmeans_labels\n",
    "cluster_summary = df.groupby(\n",
    "    'cluster')[['demand', 'temperature', 'humidity', 'hour', 'month']].mean()\n",
    "print('Cluster Characteristics:')\n",
    "print(cluster_summary)\n",
    "\n",
    "for cluster in cluster_summary.index:\n",
    "    temp = cluster_summary.loc[cluster, 'temperature']\n",
    "    demand = cluster_summary.loc[cluster, 'demand']\n",
    "    hour = cluster_summary.loc[cluster, 'hour']\n",
    "    month = cluster_summary.loc[cluster, 'month']\n",
    "    print(f'Cluster {cluster}:')\n",
    "    print(f'- Average Demand: {demand:.0f}')\n",
    "    print(f'- Average Temperature: {temp:.2f}')\n",
    "    print(f'- Average Hour: {hour:.1f}')\n",
    "    print(f'- Average Month: {month:.1f}')\n",
    "    if temp > 0.7 and hour > 12:\n",
    "        print('- Likely: High-demand hot afternoons')\n",
    "    elif temp < 0.3 and hour < 6:\n",
    "        print('- Likely: Low-demand cool nights')\n",
    "    else:\n",
    "        print('- Mixed patterns')\n",
    "    print()\n",
    "\n",
    "# Save results\n",
    "df.to_csv('clustered_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc065f0",
   "metadata": {},
   "source": [
    "Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce36edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN: 5859 clusters, 79528 noise points\n",
      "Silhouette Scores:\n",
      "K-Means: 0.138\n",
      "Hierarchical (subsample): 0.111\n",
      "DBSCAN: -0.2994783818721771\n",
      "Cluster Characteristics:\n",
      "               demand  temperature  humidity       hour     month\n",
      "cluster                                                          \n",
      "0         7309.269531     0.470318  0.909456  12.467238  6.960685\n",
      "1        10837.691406     0.701908  0.516355  14.255876  7.481997\n",
      "2         6473.314453     0.509914  0.741092   9.600948  9.791533\n",
      "3         6494.676270     0.458724  0.685714  10.840166  2.657360\n",
      "Cluster 0:\n",
      "- Average Demand: 7309\n",
      "- Average Temperature: 0.47\n",
      "- Average Hour: 12.5\n",
      "- Average Month: 7.0\n",
      "- Mixed patterns\n",
      "\n",
      "Cluster 1:\n",
      "- Average Demand: 10838\n",
      "- Average Temperature: 0.70\n",
      "- Average Hour: 14.3\n",
      "- Average Month: 7.5\n",
      "- Likely: High-demand hot afternoons\n",
      "\n",
      "Cluster 2:\n",
      "- Average Demand: 6473\n",
      "- Average Temperature: 0.51\n",
      "- Average Hour: 9.6\n",
      "- Average Month: 9.8\n",
      "- Mixed patterns\n",
      "\n",
      "Cluster 3:\n",
      "- Average Demand: 6495\n",
      "- Average Temperature: 0.46\n",
      "- Average Hour: 10.8\n",
      "- Average Month: 2.7\n",
      "- Mixed patterns\n",
      "\n",
      "Naive Forecast Metrics:\n",
      "MAE: 4375.58\n",
      "RMSE: 5533.95\n",
      "MAPE: 115.26%\n",
      "Random Forest Metrics:\n",
      "MAE: 3108.30\n",
      "RMSE: 3608.34\n",
      "MAPE: 80.17%\n",
      "Gradient Boosting Metrics:\n",
      "MAE: 3199.98\n",
      "RMSE: 3640.38\n",
      "MAPE: 84.64%\n",
      "Stacking Ensemble Metrics:\n",
      "MAE: 3111.69\n",
      "RMSE: 3583.76\n",
      "MAPE: 81.45%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('processed_data.csv')\n",
    "\n",
    "# Optimize data types to reduce memory\n",
    "df = df.astype(\n",
    "    {col: 'float32' for col in df.select_dtypes(include='float64').columns})\n",
    "\n",
    "# Select features for clustering and forecasting\n",
    "features = ['demand', 'precipIntensity', 'precipProbability', 'temperature', 'humidity',\n",
    "            'pressure', 'windSpeed', 'hour', 'day_of_week', 'month', 'year', 'anomaly']\n",
    "X = df[features]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X).astype(\n",
    "    np.float32)  # Use float32 to save memory\n",
    "\n",
    "# 1. Clustering Task\n",
    "# Dimensionality Reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=250)\n",
    "X_tsne = tsne.fit_transform(X_scaled[:5000])  # Reduced subsample for speed\n",
    "\n",
    "# Visualize PCA and t-SNE\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.3, s=10)\n",
    "plt.title(f'PCA ({explained_variance.sum():.2%} Variance)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.3, s=10)\n",
    "plt.title('t-SNE')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.tight_layout()\n",
    "plt.savefig('dimensionality_reduction.png')\n",
    "plt.close()\n",
    "\n",
    "# K-Means: Elbow Method\n",
    "inertia = []\n",
    "K = range(1, 10)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(K, inertia, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.savefig('elbow_curve.png')\n",
    "plt.close()\n",
    "\n",
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "print(f'DBSCAN: {n_clusters_dbscan} clusters, {n_noise} noise points')\n",
    "\n",
    "# Hierarchical Clustering (subsample to avoid memory error)\n",
    "sample_size = 1000\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(\n",
    "    X_scaled.shape[0], sample_size, replace=False)\n",
    "X_sample = X_scaled[sample_indices]\n",
    "\n",
    "# Dendrogram\n",
    "Z = linkage(X_sample, method='ward', metric='euclidean')\n",
    "plt.figure(figsize=(6, 4))\n",
    "dendrogram(Z, truncate_mode='level', p=5)\n",
    "plt.title('Dendrogram for Hierarchical Clustering')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.savefig('dendrogram.png')\n",
    "plt.close()\n",
    "\n",
    "# Apply hierarchical clustering to subsample\n",
    "hierarchical = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
    "hierarchical_labels_sample = hierarchical.fit_predict(X_sample)\n",
    "\n",
    "# Clustering Evaluation\n",
    "kmeans_silhouette = silhouette_score(X_scaled, kmeans_labels, sample_size=1000)\n",
    "hierarchical_silhouette = silhouette_score(\n",
    "    X_sample, hierarchical_labels_sample)\n",
    "dbscan_silhouette = None\n",
    "if n_clusters_dbscan > 1:\n",
    "    dbscan_silhouette = silhouette_score(\n",
    "        X_scaled, dbscan_labels, sample_size=1000)\n",
    "\n",
    "print(f'Silhouette Scores:')\n",
    "print(f'K-Means: {kmeans_silhouette:.3f}')\n",
    "print(f'Hierarchical (subsample): {hierarchical_silhouette:.3f}')\n",
    "print(f'DBSCAN: {dbscan_silhouette if dbscan_silhouette else \"N/A\"}')\n",
    "\n",
    "# Visualize K-Means clusters\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels,\n",
    "            cmap='viridis', alpha=0.3, s=10)\n",
    "plt.title('K-Means Clusters in PCA Space')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.savefig('kmeans_clusters.png')\n",
    "plt.close()\n",
    "\n",
    "# Cluster Interpretation\n",
    "df['cluster'] = kmeans_labels\n",
    "cluster_summary = df.groupby(\n",
    "    'cluster')[['demand', 'temperature', 'humidity', 'hour', 'month']].mean()\n",
    "print('Cluster Characteristics:')\n",
    "print(cluster_summary)\n",
    "\n",
    "for cluster in cluster_summary.index:\n",
    "    temp = cluster_summary.loc[cluster, 'temperature']\n",
    "    demand = cluster_summary.loc[cluster, 'demand']\n",
    "    hour = cluster_summary.loc[cluster, 'hour']\n",
    "    month = cluster_summary.loc[cluster, 'month']\n",
    "    print(f'Cluster {cluster}:')\n",
    "    print(f'- Average Demand: {demand:.0f}')\n",
    "    print(f'- Average Temperature: {temp:.2f}')\n",
    "    print(f'- Average Hour: {hour:.1f}')\n",
    "    print(f'- Average Month: {month:.1f}')\n",
    "    if temp > 0.7 and hour > 12:\n",
    "        print('- Likely: High-demand hot afternoons')\n",
    "    elif temp < 0.3 and hour < 6:\n",
    "        print('- Likely: Low-demand cool nights')\n",
    "    else:\n",
    "        print('- Mixed patterns')\n",
    "    print()\n",
    "\n",
    "# 2. Predictive Modeling\n",
    "# Prepare data for forecasting\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "df = df.sort_values('timestamp')\n",
    "\n",
    "# Create lag feature for previous day's demand\n",
    "df['demand_lag_24'] = df['demand'].shift(24)\n",
    "df = df.dropna()\n",
    "\n",
    "# Features for forecasting\n",
    "forecast_features = ['temperature', 'humidity', 'pressure', 'windSpeed', 'hour',\n",
    "                     'day_of_week', 'month', 'year', 'demand_lag_24']\n",
    "X_forecast = df[forecast_features].astype(np.float32)\n",
    "y_forecast = df['demand'].astype(np.float32)\n",
    "\n",
    "# Split data\n",
    "train_size = int(len(df) * 0.8)\n",
    "X_train, X_test = X_forecast[:train_size], X_forecast[train_size:]\n",
    "y_train, y_test = y_forecast[:train_size], y_forecast[train_size:]\n",
    "\n",
    "# Baseline: Naive forecast\n",
    "naive_forecast = df['demand_lag_24'][train_size:].astype(np.float32)\n",
    "naive_mae = mean_absolute_error(y_test, naive_forecast)\n",
    "naive_rmse = np.sqrt(mean_squared_error(y_test, naive_forecast))\n",
    "naive_mape = np.mean(np.abs((y_test - naive_forecast) / y_test)) * 100\n",
    "\n",
    "print('Naive Forecast Metrics:')\n",
    "print(f'MAE: {naive_mae:.2f}')\n",
    "print(f'RMSE: {naive_rmse:.2f}')\n",
    "print(f'MAPE: {naive_mape:.2f}%')\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "rf_params = {'n_estimators': [100], 'max_depth': [10]}\n",
    "rf_grid = GridSearchCV(rf, rf_params, cv=3,\n",
    "                       scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf_best = rf_grid.best_estimator_\n",
    "rf_pred = rf_best.predict(X_test)\n",
    "\n",
    "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "rf_mape = np.mean(np.abs((y_test - rf_pred) / y_test)) * 100\n",
    "\n",
    "print('Random Forest Metrics:')\n",
    "print(f'MAE: {rf_mae:.2f}')\n",
    "print(f'RMSE: {rf_rmse:.2f}')\n",
    "print(f'MAPE: {rf_mape:.2f}%')\n",
    "\n",
    "# Gradient Boosting\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "gb_params = {'n_estimators': [100], 'max_depth': [3]}\n",
    "gb_grid = GridSearchCV(gb, gb_params, cv=3, scoring='neg_mean_absolute_error')\n",
    "gb_grid.fit(X_train, y_train)\n",
    "gb_best = gb_grid.best_estimator_\n",
    "gb_pred = gb_best.predict(X_test)\n",
    "\n",
    "gb_mae = mean_absolute_error(y_test, gb_pred)\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_pred))\n",
    "gb_mape = np.mean(np.abs((y_test - gb_pred) / y_test)) * 100\n",
    "\n",
    "print('Gradient Boosting Metrics:')\n",
    "print(f'MAE: {gb_mae:.2f}')\n",
    "print(f'RMSE: {gb_rmse:.2f}')\n",
    "print(f'MAPE: {gb_mape:.2f}%')\n",
    "\n",
    "# Ensemble: Stacking\n",
    "estimators = [('rf', rf_best), ('gb', gb_best)]\n",
    "stacking = StackingRegressor(\n",
    "    estimators=estimators, final_estimator=LinearRegression(), n_jobs=-1)\n",
    "stacking.fit(X_train, y_train)\n",
    "stack_pred = stacking.predict(X_test)\n",
    "\n",
    "stack_mae = mean_absolute_error(y_test, stack_pred)\n",
    "stack_rmse = np.sqrt(mean_squared_error(y_test, stack_pred))\n",
    "stack_mape = np.mean(np.abs((y_test - stack_pred) / y_test)) * 100\n",
    "\n",
    "print('Stacking Ensemble Metrics:')\n",
    "print(f'MAE: {stack_mae:.2f}')\n",
    "print(f'RMSE: {stack_rmse:.2f}')\n",
    "print(f'MAPE: {stack_mape:.2f}%')\n",
    "\n",
    "# Visualize forecasts\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(y_test.values[:168], label='Actual', alpha=0.7)\n",
    "plt.plot(stack_pred[:168], label='Stacking Predicted', alpha=0.7)\n",
    "plt.plot(naive_forecast.values[:168], label='Naive', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Demand (First Week)')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Demand (MWh)')\n",
    "plt.legend()\n",
    "plt.savefig('forecast_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Save results\n",
    "df.to_csv('clustered_and_forecasted_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
